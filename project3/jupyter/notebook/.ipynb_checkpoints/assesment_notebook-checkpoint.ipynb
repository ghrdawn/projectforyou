{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    appName(\"pyspark-1\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Job ID: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Posting Type: string (nullable = true)\n",
      " |-- # Of Positions: string (nullable = true)\n",
      " |-- Business Title: string (nullable = true)\n",
      " |-- Civil Service Title: string (nullable = true)\n",
      " |-- Title Code No: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Job Category: string (nullable = true)\n",
      " |-- Full-Time/Part-Time indicator: string (nullable = true)\n",
      " |-- Salary Range From: string (nullable = true)\n",
      " |-- Salary Range To: string (nullable = true)\n",
      " |-- Salary Frequency: string (nullable = true)\n",
      " |-- Work Location: string (nullable = true)\n",
      " |-- Division/Work Unit: string (nullable = true)\n",
      " |-- Job Description: string (nullable = true)\n",
      " |-- Minimum Qual Requirements: string (nullable = true)\n",
      " |-- Preferred Skills: string (nullable = true)\n",
      " |-- Additional Information: string (nullable = true)\n",
      " |-- To Apply: string (nullable = true)\n",
      " |-- Hours/Shift: string (nullable = true)\n",
      " |-- Work Location 1: string (nullable = true)\n",
      " |-- Recruitment Contact: string (nullable = true)\n",
      " |-- Residency Requirement: string (nullable = true)\n",
      " |-- Posting Date: string (nullable = true)\n",
      " |-- Post Until: string (nullable = true)\n",
      " |-- Posting Updated: string (nullable = true)\n",
      " |-- Process Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_frequency(df: DataFrame) -> list:\n",
    "    row_list = df.select('Salary Frequency').distinct().collect()\n",
    "    return [row['Salary Frequency'] for row in row_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whats the number of jobs posting per category (Top 10)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top10_jobs_posting_per_category(df: DataFrame) -> list:\n",
    "    job_posting_per_category = df.groupBy('Job Category').count().orderBy(col('count').desc())\n",
    "    return [(row[0], row[1]) for row in job_posting_per_category.take(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Engineering, Architecture, & Planning', 504),\n",
       " ('Technology, Data & Innovation', 313),\n",
       " ('Legal Affairs', 226),\n",
       " ('Public Safety, Inspections, & Enforcement', 182),\n",
       " ('Building Operations & Maintenance', 181),\n",
       " ('Finance, Accounting, & Procurement', 169),\n",
       " ('Administration & Human Resources', 134),\n",
       " ('Constituent Services & Community Programs', 129),\n",
       " ('Health', 125),\n",
       " ('Policy, Research & Analysis', 124)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top10_jobs_posting_per_category(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = [('A', 'Annual'), ('B', 'Daily')]\n",
    "expected_result = ['Annual', 'Daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_salary_frequency(mock_data: list, \n",
    "                              expected_result: list,\n",
    "                              schema: list = ['id', 'Salary Frequency']):  \n",
    "    mock_df = spark.createDataFrame(data = mock_data, schema = schema)\n",
    "    assert get_salary_frequency(mock_df) == expected_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test : get_top10_jobs_posting_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = [('C1', 1), ('C1', 2), ('C1', 3),\n",
    "            ('C2', 4), ('C2', 5),\n",
    "            ('C3', 6), ('C3', 7), ('C3', 8),\n",
    "            ('C4', 9), ('C4', 10), ('C4', 11),\n",
    "            ('C5', 12), ('C5', 13), ('C5', 14),('C5', 15),\n",
    "            ('C6', 16), ('C6', 17), ('C6', 18),\n",
    "            ('C7', 19), ('C7', 20),\n",
    "            ('C8', 21), ('C8', 22), ('C8', 23),\n",
    "            ('C9', 24), ('C9', 25), ('C9', 26),('C9', 27), ('C9', 28),\n",
    "            ('C10', 29), ('C10', 30), ('C10', 31),\n",
    "            ('C11', 32)]\n",
    "expected_result = [('C9', 5),\n",
    "                  ('C5', 4),\n",
    "                  ('C6', 3),\n",
    "                  ('C4', 3),\n",
    "                  ('C10', 3),\n",
    "                  ('C3', 3),\n",
    "                  ('C8', 3),\n",
    "                  ('C1', 3),\n",
    "                  ('C7', 2),\n",
    "                  ('C2', 2)]\n",
    "\n",
    "def test_get_top10_jobs_posting_per_category(mock_data: list, \n",
    "                              expected_result: list,\n",
    "                              schema: list = ['Job Category', 'Job Id']):\n",
    "    mock_df = spark.createDataFrame(data = mock_data, schema = schema)\n",
    "    assert sorted(get_top10_jobs_posting_per_category(mock_df)) == sorted(expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = list = ['Job Category', 'Job Id']\n",
    "test_get_top10_jobs_posting_per_category(mock_data, expected_result, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
